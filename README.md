**A Data Scaling Law of a Manifold's Resolution**
[Click here to view the paper (PDF)](A_Data_Scaling_Law_of_a_Manifold_s_Resolution.pdf)

*Author(s)*: Abiel J. Kim

*Date*: April 2025

*Keywords*: Neural Scaling Law, Data, Machine Learning, Deep Learning, Statistical Learning Theory, Differenial Geometry, Manifold Learning, Intrinsic Dimension, Bound, Lipschitz, Hypothesis Class, Probability Theory

**ABSTRACT**

It has been empirically observed that neural network performance generally assumes the power law formulation for the scaling of its training dataset. 
The experimental evidence is compelling, but the theoretical frontier remains exploratory with respect to a mathematical origin of the observed power law. 
This paper introduces a mathematical framework of the geometric kind that enables the emergence of a bounding of the data scaling law. 
The mathematical framework is predicated on the manifold conjecture and interprets the scaling of a dataset as a finer approximation to the true data manifold space. 
The equations indicate that model loss, $L$, indeed scales as a power law with $L \propto D^{-1/d}$ for the data manifold's intrinsic dimensionality, $d$.

**INTRODUCTION**

As per the question of the neural scaling laws posed by researchers at OpenAI, Kaplan et al. (2020) proposed the empirically observed power law formulations for model capacity (N), dataset size (D), and compute (C).
And as discussed in lecture, we saw the empirical literature surrounding the proposed power law formulation for the scaling of D. 
We also discussed the idea that there does not yet exist a universally accepted theoretical solution regarding the mathematical origin of such observed power laws.

The objective of this paper is to discover a theoretical upper bound for the data scaling law from first principles which has been empirically fitted as a power law formulation. 
The implication of such a discovery would provide either confirming or disconfirming evidence for the fitting of the power law with respect to data scaling. 
In order to achieve this, I will provide firstly a geometric intuition of my framework. 
Then, I will provide a more detailed mathematical derivation that leverages some of the mathematical properties of the geometric framework which leverages the manifold conjecture. 
Note that the manifold conjecture is a mainstream approach in terms of theoretical analysis in the machine learning field. 

**MANIFOLD RESOLUTION THEOREM**

**The Geometric Intuition**

This geometric framework assumes the manifold conjecture, in which a given data set corresponds to a sampling of an underlying $d$-manifold in multidimensional space. i.e. the linear regression architecture assumes that the dataset is a sampling of a linear $n$-manifold in $n+1$ space with the trivial case of a line manifold embedded in $2$ dimensional space. The decision boundaries correspond to hyperplanes that subdivide the $n$-manifold, and it is the model's objective to distinguish between these hypersubspaces by minimizing the loss between prediction and truth.

The natural extension for a more complex dataset is the definition of a more complicated $d$-manifold geometry. The feature space may no longer be linearly correlated, and hence the underlying manifold structure may assume a highly irregular, nonlinear structure. \textit{We are permitted to interpret the dataset as a discrete sampling of the underlying lower-dimensional $d$-manifold} with the underlying manifold being a continuously defined structure. The objective of the deep neural network should then be the subdivision of the $d$-manifold into hypercubic regions that correspond to class membership mappings.

In the limit, as the size of the dataset, $D$, scales, observe that a better approximation to the underlying $d$-manifold structure is attained. In other words, as $D$ scales, the resolution of the $d$-manifold structure increases and the structure clarifies. Therefore, realize that if $D$ approaches infinity, then a perfect representation of the $d$-manifold is achieved.

**Definitions and Notation**

Let us define the input space, $\mathcal{X} \subseteq \mathbb{R}^N$, of dimension $dim(\mathcal{X})=N$. Embedded within $\mathcal{X}$ there exists the $d$-manifold structure, $\mathcal{M} \subseteq \mathbb{R}^N$, of intrinsic dimensionality $dim(\mathcal{M})=d$ that is smooth and compact with $d<<N$. The consequence of compactness is the assertion of a finite volume $V_{\mathcal{M}} < \infty$ that the manifold inhabits. Further, we assume that the dataset $\{ x_1, x_2, \dots, x_D \} \in \mathcal{M}$ gets sampled from the surface of the data manifold at i.i.d. with uniform probability $p(x)$.

Next, we shall define the hypothesis class, H, of Lipschitz functions that maps a sample on the surface of â„³ to a real number expressed as f : â„³ â†’ â„ and |f(x)âˆ’f(z)| â‰¤ Lâ€–xâˆ’zâ€–, âˆ€ f âˆˆ H for a real positive constant L.
The predictive function fÌ‚ âˆˆ H : â„³ â†’ â„ corresponds to our learned model mapping.
The true function fâ˜… : â„³ â†’ â„ represents the theoretically perfect mapping which may exist outside of the hypothesis class such that fâ˜… âˆˆ H or fâ˜… âˆ‰ H.
However, we also assume that the true function is Lipschitz, thus |fâ˜…(x)âˆ’fâ˜…(z)| â‰¤ Kâ€–xâˆ’zâ€– for a real positive constant K.
The Lipschitz constraints imposed upon H and fâ˜… reduce the set of all possible functions to those that do not oscillate rapidly between arbitrary pairs of neighboring data points upon the surface of the data manifold.

The true risk R(f) for some arbitrary f âˆˆ H is the expected MSE between f âˆˆ H and the true function fâ˜….
If we assume that data is sampled i.i.d. from the d-manifold surface at uniform probability p(x) then we formulate true risk as the integral:
R(f) = ð”¼[(f(x) âˆ’ fâ˜…(x))Â²] = âˆ«_â„³ (f(x) âˆ’ fâ˜…(x))Â² p(x) dV_â„³
for some f âˆˆ H where x lies on the surface of â„³.
The empirical risk, RÌ‚_D(f) for some f âˆˆ H, is the average MSE between f and the true function fâ˜… over D data points.
This is equivalent to the training error and can be simply expressed as:
RÌ‚_D(f) = (1/D) Î£_{i â‰¤ D} (f(xáµ¢) âˆ’ fâ˜…(xáµ¢))Â².

Correspondingly, the true minimizer f_Fâ˜… âˆˆ H is the optimal function with minimum true risk such that f_Hâ˜… = argmin_{âˆ€ f âˆˆ H} R(f).
Then, we shall define the empirical minimizer fÌ‚_D âˆˆ H that corresponds to the optimal function with minimum empirical risk over D discrete points {xâ‚, xâ‚‚, â€¦, x_D} âˆˆ â„³ such that fÌ‚_D = argmin_{âˆ€ f âˆˆ H} RÌ‚_D(f).

If f_Hâ˜… is the best approximation from H to fâ˜… over the population dataset and fÌ‚_D is the best approximation from H to fâ˜… over D sampled data points, then we must discover and bound the excess risk R(fÌ‚_D) âˆ’ R(f_Hâ˜…) as D â†’ âˆž from first principles.

**Reiteration of Key Assumptions**

Assumption 1: â„³ is smooth and compact i.e. â„³ is differentiable and bounding of a finite volume.
Assumption 2: The dataset {xâ‚, xâ‚‚, â€¦, x_D} âˆˆ â„³ is distributed uniformly across the data manifold. When sampling, we assume points are taken with a uniform probability distribution at i.i.d.
Assumption 3: The Lipschitz hypothesis class H comprises smooth, non-jagged function surfaces. The true function fâ˜… is also Lipschitz.

**Modeling the Data Manifold Resolution**

As D increases, data points inhabit the data manifold's volume at increasing resolution. If V_â„³ is finite and we have D sample points that are uniformly distributed across V_â„³ then each data point inhabits a region of V_â„³ / D space on average.

The volume of a data point in â„³ can be modeled as the volume of a d-ball in d space, Ráµˆ V_d, where V_d is the volume of the unit d-ball and Ráµˆ is its radius. For instance: If d = 2 then V_d = Ï€, if d = 3 then V_d = 4Ï€/3, and so forth.

It therefore follows that we can express the average volume that each data point occupies as Ráµˆ V_d = V_â„³ / D. We are permitted to express it in this way since we assume the uniform distribution of data points across the manifold. Solve for radius to find R = (V_â„³ / V_d)^(1/d) D^(âˆ’1/d).

Furthermore, assumption 2 permits us to interpret R as an approximation to the typical radius. Therefore, we may use R to approximate the average distance between neighboring data points upon the surface of the manifold. Finally, express typical radius as R = Ï† D^(âˆ’1/d) which gives us a measure of the manifold's resolution.

Note that we have modeled the volume of the inhabited region by each data point as a hypersphere and not some other geometry such as a hypercube. This is justified because whether the region of each data point is modeled as a hypercube or any other arbitrary shape in d-space, the formulation for R typically yields the same structure: D^(âˆ’1/d) multiplied by some constant. It therefore follows that as D â†’ âˆž, the average-volume approximations modeled with different hyper-geometries converge to the same typical radius. The decision of a hypersphere is chosen more for its convenience.

[Click here to view the paper (PDF)](A_Data_Scaling_Law_of_a_Manifold_s_Resolution.pdf)

> **Status:** This manuscript may be subject to future revisions.

> **Material:** This paper was originally written as part of coursework at Simon Fraser University (SFU).

> **Review:** This work received informal feedback from academic researchers at the University of Illinois Urbanaâ€“Champaign (UIUC).
